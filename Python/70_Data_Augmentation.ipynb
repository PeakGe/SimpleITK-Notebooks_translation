{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Data Augmentation for Deep Learning</h1>\n",
    "\n",
    "本笔记本说明了使用SimpleITK为深度学习执行数据增强。 请注意，编写代码时，相关功能无需修改即可用于2D和3D图像。\n",
    "\n",
    "数据增强是一种基于模型的方法，用于扩大训练集。 要解决的问题是原始数据集不足以代表一般的图像群。 因此，如果我们只对原始数据集进行训练，那么得到的网络将不能很好地推广到总体（过度拟合）。\n",
    "\n",
    "使用一般种群和现有数据集中发现的变异模型，我们生成额外的图像，以期捕获种群变异性。 请注意，如果您使用的模型不正确，则可能会造成伤害，您正在生成一般人群中未发生的观察，并且正在优化适合它们的函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib notebook\n",
    "import gui\n",
    "\n",
    "#utility方法从Girder存储库下载数据或者已经下载的数据返回用于从磁盘读取的文件名（缓存数据）\n",
    "%run update_path_to_download_script\n",
    "from downloaddata import fetch_data as fdata\n",
    "\n",
    "OUTPUT_DIR = 'Output'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before we start, a word of caution\n",
    "无论何时采样，都存在混叠(噪声)的可能性（奈奎斯特定理）。\n",
    "\n",
    "在许多情况下，准备用于深度学习网络的数据被重新采样到固定大小。 当我们通过空间变换执行数据增强时，我们还执行重采样。\n",
    "\n",
    "诚然，为了说明这一点，下面的示例有些夸张，但它提醒您，您可能需要考虑在重新采样之前对图像进行平滑处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The image we will resample (a grid).\n",
    "grid_image = sitk.GridSource(outputPixelType=sitk.sitkUInt16, size=(512,512), \n",
    "                             sigma=(0.1,0.1), gridSpacing=(20.0,20.0))\n",
    "sitk.Show(grid_image, \"original grid image\")\n",
    "\n",
    "# The spatial definition of the images we want to use in a deep learning framework (smaller than the original). \n",
    "new_size = [100, 100]\n",
    "reference_image = sitk.Image(new_size, grid_image.GetPixelIDValue())\n",
    "reference_image.SetOrigin(grid_image.GetOrigin())\n",
    "reference_image.SetDirection(grid_image.GetDirection())\n",
    "reference_image.SetSpacing([sz*spc/nsz for nsz,sz,spc in zip(new_size, grid_image.GetSize(), grid_image.GetSpacing())])\n",
    "\n",
    "# Resample without any smoothing.\n",
    "sitk.Show(sitk.Resample(grid_image, reference_image) , \"resampled without smoothing\")\n",
    "\n",
    "# Resample after Gaussian smoothing.\n",
    "sitk.Show(sitk.Resample(sitk.SmoothingRecursiveGaussian(grid_image, 2.0), reference_image), \"resampled with smoothing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "您可以使用原始的3D图像或原始卷中的2D切片在笔记本上工作。要实现后者，只需取消下面单元格中的行注释。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [sitk.ReadImage(fdata(\"nac-hncma-atlas2013-Slicer4Version/Data/A1_grayT1.nrrd\")),\n",
    "        sitk.ReadImage(fdata(\"vm_head_mri.mha\")),\n",
    "        sitk.ReadImage(fdata(\"head_mr_oriented.mha\"))]\n",
    "# Comment out the following line if you want to work in 3D. Note that in 3D some of the notebook visualizations are \n",
    "# disabled. \n",
    "data = [data[0][:,160,:], data[1][:,:,17], data[2][:,:,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disp_images(images, fig_size, wl_list=None):\n",
    "    if images[0].GetDimension()==2:\n",
    "        gui.multi_image_display2D(image_list=images, figure_size=fig_size, window_level_list=wl_list)\n",
    "    else:\n",
    "        gui.MultiImageDisplay(image_list=images, figure_size=fig_size, window_level_list=wl_list)\n",
    "    \n",
    "disp_images(data, fig_size=(6,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原始数据通常需要修改。 在这个例子中，我们想裁剪图像，以便我们只保留有信息的区域。 我们可以使用适当的阈值轻松地分离前景和背景，在我们的例子中，我们使用Otsu的阈值选择方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_based_crop(image):\n",
    "    \"\"\"\n",
    "    Use Otsu's threshold estimator to separate background and foreground. In medical imaging the background is\n",
    "    usually air. Then crop the image using the foreground's axis aligned bounding box.\n",
    "    Args:\n",
    "        image (SimpleITK image): An image where the anatomy and background intensities form a bi-modal distribution\n",
    "                                 (the assumption underlying Otsu's method.)\n",
    "    Return:\n",
    "        Cropped image based on foreground's axis aligned bounding box.                                 \n",
    "    \"\"\"\n",
    "    # Set pixels that are in [min_intensity,otsu_threshold] to inside_value, values above otsu_threshold are\n",
    "    # set to outside_value. The anatomy has higher intensity values than the background, so it is outside.\n",
    "    inside_value = 0\n",
    "    outside_value = 255\n",
    "    label_shape_filter = sitk.LabelShapeStatisticsImageFilter()\n",
    "    label_shape_filter.Execute( sitk.OtsuThreshold(image, inside_value, outside_value) )\n",
    "    bounding_box = label_shape_filter.GetBoundingBox(outside_value)\n",
    "    # The bounding box's first \"dim\" entries are the starting index and last \"dim\" entries the size\n",
    "    return sitk.RegionOfInterest(image, bounding_box[int(len(bounding_box)/2):], bounding_box[0:int(len(bounding_box)/2)])\n",
    "    \n",
    "\n",
    "modified_data = [threshold_based_crop(img) for img in data]\n",
    "\n",
    "disp_images(modified_data, fig_size=(6,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此时我们选择要使用的图像，如果要使用原始数据，请跳过以下单元格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = modified_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentation using spatial transformations\n",
    "\n",
    "We next illustrate the generation of images by specifying a list of transformation parameter values representing a sampling of the transformation's parameter space.\n",
    "\n",
    "The code below is agnostic to the specific transformation and it is up to the user to specify a valid list of transformation parameters (correct number of parameters and correct order). To learn more about the spatial transformations supported by SimpleITK you can explore the [Transforms notebook](22_Transforms.ipynb).\n",
    "\n",
    "In most cases we can easily specify a regular grid in parameter space by specifying ranges of values for each of the parameters. In some cases specifying parameter values may be less intuitive (i.e. versor representation of rotation).\n",
    "\n",
    "接下来，我们通过指定表示变换参数空间的采样的变换参数值列表来说明图像的生成。\n",
    "\n",
    "下面的代码与特定转换无关，用户可以指定转换参数的有效列表（正确的参数数量和正确的顺序）。 要了解有关SimpleITK支持的Transforms notebook的更多信息，您可以浏览Transforms笔记本。\n",
    "\n",
    "在大多数情况下，我们可以通过指定每个参数的值范围，轻松地在参数空间中指定常规网格。 在某些情况下，指定参数值可能不太直观（即，旋转的相反表示）。\n",
    "\n",
    "## Utility methods\n",
    "Utilities for sampling a parameter space using a regular grid in a convenient manner (special care for 3D similarity).\n",
    "\n",
    "用于以方便的方式使用规则网格对参数空间进行采样的实用程序（特别注意3D相似性）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_space_regular_grid_sampling(*transformation_parameters):\n",
    "    '''\n",
    "    Create a list representing a regular sampling of the parameter space.     \n",
    "    Args:\n",
    "        *transformation_paramters : two or more numpy ndarrays representing parameter values. The order \n",
    "                                    of the arrays should match the ordering of the SimpleITK transformation \n",
    "                                    parameterization (e.g. Similarity2DTransform: scaling, rotation, tx, ty)\n",
    "    Return:\n",
    "        List of lists representing the regular grid sampling.\n",
    "        \n",
    "    Examples:\n",
    "        #parameterization for 2D translation transform (tx,ty): [[1.0,1.0], [1.5,1.0], [2.0,1.0]]\n",
    "        >>>> parameter_space_regular_grid_sampling(np.linspace(1.0,2.0,3), np.linspace(1.0,1.0,1))        \n",
    "    '''\n",
    "    return [[np.asscalar(p) for p in parameter_values] \n",
    "            for parameter_values in np.nditer(np.meshgrid(*transformation_parameters))]\n",
    "\n",
    "def similarity3D_parameter_space_regular_sampling(thetaX, thetaY, thetaZ, tx, ty, tz, scale):\n",
    "    '''\n",
    "    Create a list representing a regular sampling of the 3D similarity transformation parameter space. As the\n",
    "    SimpleITK rotation parameterization uses the vector portion of a versor we don't have an \n",
    "    intuitive way of specifying rotations. We therefor use the ZYX Euler angle parametrization and convert to\n",
    "    versor.\n",
    "    Args:\n",
    "        thetaX, thetaY, thetaZ: numpy ndarrays with the Euler angle values to use.\n",
    "        tx, ty, tz: numpy ndarrays with the translation values to use.\n",
    "        scale: numpy array with the scale values to use.\n",
    "    Return:\n",
    "        List of lists representing the parameter space sampling (vx,vy,vz,tx,ty,tz,s).\n",
    "    '''\n",
    "    return [list(eul2quat(parameter_values[0],parameter_values[1], parameter_values[2])) + \n",
    "            [np.asscalar(p) for p in parameter_values[3:]] for parameter_values in np.nditer(np.meshgrid(thetaX, thetaY, thetaZ, tx, ty, tz, scale))]\n",
    "    \n",
    "\n",
    "def eul2quat(ax, ay, az, atol=1e-8):\n",
    "    '''\n",
    "    Translate between Euler angle (ZYX) order and quaternion representation of a rotation.\n",
    "    Args:\n",
    "        ax: X rotation angle in radians.\n",
    "        ay: Y rotation angle in radians.\n",
    "        az: Z rotation angle in radians.\n",
    "        atol: tolerance used for stable quaternion computation (qs==0 within this tolerance).\n",
    "    Return:\n",
    "        Numpy array with three entries representing the vectorial component of the quaternion.\n",
    "\n",
    "    '''\n",
    "    # Create rotation matrix using ZYX Euler angles and then compute quaternion using entries.\n",
    "    cx = np.cos(ax)\n",
    "    cy = np.cos(ay)\n",
    "    cz = np.cos(az)\n",
    "    sx = np.sin(ax)\n",
    "    sy = np.sin(ay)\n",
    "    sz = np.sin(az)\n",
    "    r=np.zeros((3,3))\n",
    "    r[0,0] = cz*cy \n",
    "    r[0,1] = cz*sy*sx - sz*cx\n",
    "    r[0,2] = cz*sy*cx+sz*sx     \n",
    "\n",
    "    r[1,0] = sz*cy \n",
    "    r[1,1] = sz*sy*sx + cz*cx \n",
    "    r[1,2] = sz*sy*cx - cz*sx\n",
    "\n",
    "    r[2,0] = -sy   \n",
    "    r[2,1] = cy*sx             \n",
    "    r[2,2] = cy*cx\n",
    "\n",
    "    # Compute quaternion: \n",
    "    qs = 0.5*np.sqrt(r[0,0] + r[1,1] + r[2,2] + 1)\n",
    "    qv = np.zeros(3)\n",
    "    # If the scalar component of the quaternion is close to zero, we\n",
    "    # compute the vector part using a numerically stable approach\n",
    "    if np.isclose(qs,0.0,atol): \n",
    "        i= np.argmax([r[0,0], r[1,1], r[2,2]])\n",
    "        j = (i+1)%3\n",
    "        k = (j+1)%3\n",
    "        w = np.sqrt(r[i,i] - r[j,j] - r[k,k] + 1)\n",
    "        qv[i] = 0.5*w\n",
    "        qv[j] = (r[i,j] + r[j,i])/(2*w)\n",
    "        qv[k] = (r[i,k] + r[k,i])/(2*w)\n",
    "    else:\n",
    "        denom = 4*qs\n",
    "        qv[0] = (r[2,1] - r[1,2])/denom;\n",
    "        qv[1] = (r[0,2] - r[2,0])/denom;\n",
    "        qv[2] = (r[1,0] - r[0,1])/denom;\n",
    "    return qv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create reference domain\n",
    "All input images will be resampled onto the reference domain.\n",
    "\n",
    "This domain is defined by two constraints: the number of pixels per dimension and the physical size we want the reference domain to occupy. The former is associated with the computational constraints of deep learning where using a small number of pixels is desired. The later is associated with the SimpleITK concept of an image, it occupies a region in physical space which should be large enough to encompass the object of interest.\n",
    "\n",
    "所有输入图像将重新采样到参考域。\n",
    "\n",
    "该域由两个约束定义：每个维度的像素数和我们希望引用域占用的物理大小。 前者与深度学习的计算约束相关联，其中期望使用少量像素。 后者与图像的SimpleITK概念相关联，它占据物理空间中的区域，该区域应足够大以包含感兴趣的对象。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension = data[0].GetDimension()\n",
    "\n",
    "# Physical image size corresponds to the largest physical size in the training set, or any other arbitrary size.\n",
    "reference_physical_size = np.zeros(dimension)\n",
    "for img in data:\n",
    "    reference_physical_size[:] = [(sz-1)*spc if sz*spc>mx  else mx for sz,spc,mx in zip(img.GetSize(), img.GetSpacing(), reference_physical_size)]\n",
    "\n",
    "# Create the reference image with a zero origin, identity direction cosine matrix and dimension     \n",
    "reference_origin = np.zeros(dimension)\n",
    "reference_direction = np.identity(dimension).flatten()\n",
    "\n",
    "# Select arbitrary number of pixels per dimension, smallest size that yields desired results \n",
    "# or the required size of a pretrained network (e.g. VGG-16 224x224), transfer learning. This will \n",
    "# often result in non-isotropic pixel spacing.\n",
    "reference_size = [128]*dimension \n",
    "reference_spacing = [ phys_sz/(sz-1) for sz,phys_sz in zip(reference_size, reference_physical_size) ]\n",
    "\n",
    "# Another possibility is that you want isotropic pixels, then you can specify the image size for one of\n",
    "# the axes and the others are determined by this choice. Below we choose to set the x axis to 128 and the\n",
    "# spacing set accordingly. \n",
    "# Uncomment the following lines to use this strategy.\n",
    "#reference_size_x = 128\n",
    "#reference_spacing = [reference_physical_size[0]/(reference_size_x-1)]*dimension\n",
    "#reference_size = [int(phys_sz/(spc) + 1) for phys_sz,spc in zip(reference_physical_size, reference_spacing)]\n",
    "\n",
    "reference_image = sitk.Image(reference_size, data[0].GetPixelIDValue())\n",
    "reference_image.SetOrigin(reference_origin)\n",
    "reference_image.SetSpacing(reference_spacing)\n",
    "reference_image.SetDirection(reference_direction)\n",
    "\n",
    "# Always use the TransformContinuousIndexToPhysicalPoint to compute an indexed point's physical coordinates as \n",
    "# this takes into account size, spacing and direction cosines. For the vast majority of images the direction \n",
    "# cosines are the identity matrix, but when this isn't the case simply multiplying the central index by the \n",
    "# spacing will not yield the correct coordinates resulting in a long debugging session. \n",
    "reference_center = np.array(reference_image.TransformContinuousIndexToPhysicalPoint(np.array(reference_image.GetSize())/2.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generation\n",
    "\n",
    "Once we have a reference domain we can augment the data using any of the SimpleITK global domain transformations. In this notebook we use a similarity transformation (the generate_images function is agnostic to this specific choice).\n",
    "\n",
    "Note that you also need to create the labels for your augmented images. If these are just classes then your processing is minimal. If you are dealing with segmentation you will also need to transform the segmentation labels so that they match the transformed image. The following function easily accommodates for this, just provide the labeled image as input and use the sitk.sitkNearestNeighbor interpolator so that you do not introduce labels that were not in the original segmentation.   \n",
    "\n",
    "一旦我们有了一个参考域，我们就可以使用任何SimpleITK全局域转换来扩充数据。 在这个笔记本中，我们使用相似变换（generate_images函数对这个特定的选择是不可知的）。\n",
    "\n",
    "请注意，您还需要为增强图像创建标签。 如果这些只是类，那么您的处理是最小的。 如果您正在处理分割，您还需要转换分割标签，使它们与转换后的图像匹配。 以下功能很容易适应这种情况，只需提供标记图像作为输入并使用sitk.sitkNearestNeighbor插值器，这样就不会引入原始分段中没有的标签。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_images_spatial(original_image, reference_image, T0, T_aug, transformation_parameters,\n",
    "                    output_prefix, output_suffix,\n",
    "                    interpolator = sitk.sitkLinear, default_intensity_value = 0.0):\n",
    "    '''\n",
    "    Generate the resampled images based on the given transformations.\n",
    "    Args:\n",
    "        original_image (SimpleITK image): The image which we will resample and transform.\n",
    "        reference_image (SimpleITK image): The image onto which we will resample.\n",
    "        T0 (SimpleITK transform): Transformation which maps points from the reference image coordinate system \n",
    "            to the original_image coordinate system.\n",
    "        T_aug (SimpleITK transform): Map points from the reference_image coordinate system back onto itself using the\n",
    "               given transformation_parameters. The reason we use this transformation as a parameter\n",
    "               is to allow the user to set its center of rotation to something other than zero.\n",
    "        transformation_parameters (List of lists): parameter values which we use T_aug.SetParameters().\n",
    "        output_prefix (string): output file name prefix (file name: output_prefix_p1_p2_..pn_.output_suffix).\n",
    "        output_suffix (string): output file name suffix (file name: output_prefix_p1_p2_..pn_.output_suffix).\n",
    "        interpolator: One of the SimpleITK interpolators.\n",
    "        default_intensity_value: The value to return if a point is mapped outside the original_image domain.\n",
    "    '''\n",
    "    all_images = [] # Used only for display purposes in this notebook.\n",
    "    for current_parameters in transformation_parameters:\n",
    "        T_aug.SetParameters(current_parameters)        \n",
    "        # Augmentation is done in the reference image space, so we first map the points from the reference image space\n",
    "        # back onto itself T_aug (e.g. rotate the reference image) and then we map to the original image space T0.\n",
    "        T_all = sitk.Transform(T0)\n",
    "        T_all.AddTransform(T_aug)\n",
    "        aug_image = sitk.Resample(original_image, reference_image, T_all,\n",
    "                                  interpolator, default_intensity_value)\n",
    "        sitk.WriteImage(aug_image, output_prefix + '_' + \n",
    "                        '_'.join(str(param) for param in current_parameters) +'_.' + output_suffix)\n",
    "         \n",
    "        all_images.append(aug_image) # Used only for display purposes in this notebook.\n",
    "    return all_images # Used only for display purposes in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can use the generate_images function we need to compute the transformation which will map points between the reference image and the current image as shown in the code cell below.\n",
    "\n",
    "Note that it is very easy to generate large amounts of data, the calls to np.linspace with m parameters each having n values results in nm images, so don't forget that these images are also saved to disk. If you run the code below for 3D data you will generate 6561 volumes (37 parameter combinations times 3 volumes).\n",
    "\n",
    "在我们可以使用generate_images函数之前，我们需要计算转换，该转换将映射参考图像和当前图像之间的点，如下面的代码单元格所示。\n",
    "\n",
    "请注意，生成大量数据非常容易，使用m个参数调用np.linspace每个都有n个值会产生nm图像，所以不要忘记这些图像也会保存到磁盘上。 如果您为3D数据运行以下代码，您将生成6561卷（37个参数组合乘以3个卷）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_transform = sitk.Similarity2DTransform() if dimension==2 else sitk.Similarity3DTransform()\n",
    "\n",
    "all_images = []\n",
    "\n",
    "for index,img in enumerate(data):\n",
    "    # Transform which maps from the reference_image to the current img with the translation mapping the image\n",
    "    # origins to each other.\n",
    "    transform = sitk.AffineTransform(dimension)\n",
    "    transform.SetMatrix(img.GetDirection())\n",
    "    transform.SetTranslation(np.array(img.GetOrigin()) - reference_origin)\n",
    "    # Modify the transformation to align the centers of the original and reference image instead of their origins.\n",
    "    centering_transform = sitk.TranslationTransform(dimension)\n",
    "    img_center = np.array(img.TransformContinuousIndexToPhysicalPoint(np.array(img.GetSize())/2.0))\n",
    "    centering_transform.SetOffset(np.array(transform.GetInverse().TransformPoint(img_center) - reference_center))\n",
    "    centered_transform = sitk.Transform(transform)\n",
    "    centered_transform.AddTransform(centering_transform)\n",
    "\n",
    "    # Set the augmenting transform's center so that rotation is around the image center.\n",
    "    aug_transform.SetCenter(reference_center)\n",
    "    \n",
    "    if dimension == 2:\n",
    "        # The parameters are scale (+-10%), rotation angle (+-10 degrees), x translation, y translation\n",
    "        transformation_parameters_list = parameter_space_regular_grid_sampling(np.linspace(0.9,1.1,3),\n",
    "                                                                               np.linspace(-np.pi/18.0,np.pi/18.0,3),\n",
    "                                                                               np.linspace(-10,10,3),\n",
    "                                                                               np.linspace(-10,10,3))\n",
    "    else:    \n",
    "        transformation_parameters_list = similarity3D_parameter_space_regular_sampling(np.linspace(-np.pi/18.0,np.pi/18.0,3),\n",
    "                                                                                       np.linspace(-np.pi/18.0,np.pi/18.0,3),\n",
    "                                                                                       np.linspace(-np.pi/18.0,np.pi/18.0,3),\n",
    "                                                                                       np.linspace(-10,10,3),\n",
    "                                                                                       np.linspace(-10,10,3),\n",
    "                                                                                       np.linspace(-10,10,3),\n",
    "                                                                                       np.linspace(0.9,1.1,3))\n",
    "    generated_images = augment_images_spatial(img, reference_image, centered_transform, \n",
    "                                       aug_transform, transformation_parameters_list, \n",
    "                                       os.path.join(OUTPUT_DIR, 'spatial_aug'+str(index)), 'mha')\n",
    "    \n",
    "    if dimension==2: # in 2D we join all of the images into a 3D volume which we use for display.\n",
    "        all_images.append(sitk.JoinSeries(generated_images))\n",
    "# If working in 2D, display the resulting set of images.    \n",
    "if dimension==2:\n",
    "    gui.MultiImageDisplay(image_list=all_images, shared_slider=True, figure_size=(6,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about flipping\n",
    "\n",
    "Reflection using SimpleITK can be done in one of several ways:\n",
    "1. Use an affine transform with the matrix component set to a reflection matrix. The columns of the matrix correspond to the $\\mathbf{x}, \\mathbf{y}$ and $\\mathbf{z}$ axes. The reflection matrix is constructed using the plane, 3D,  or axis, 2D, which we want to reflect through with the standard basis vectors, $\\mathbf{e}_i, \\mathbf{e}_j$, and the remaining basis vector set to $-\\mathbf{e}_k$.  \n",
    "    * Reflection about $xy$ plane: $[\\mathbf{e}_1, \\mathbf{e}_2, -\\mathbf{e}_3]$.\n",
    "    * Reflection about $xz$ plane: $[\\mathbf{e}_1, -\\mathbf{e}_2, \\mathbf{e}_3]$.\n",
    "    * Reflection about $yz$ plane: $[-\\mathbf{e}_1, \\mathbf{e}_2, \\mathbf{e}_3]$.\n",
    "2. Use the native slicing operator(e.g. img[:,::-1,:]), or the FlipImageFilter after the image is resampled onto the reference image grid. \n",
    "\n",
    "We prefer option 1 as it is computationally more efficient. It combines all transformation prior to resampling, while the other approach performs resampling onto the reference image grid followed by the reflection operation. An additional difference is that using slicing or the FlipImageFilter will also modify the image origin while the resampling approach keeps the spatial location of the reference image origin intact. This minor difference is of no concern in deep learning as the content of the images is the same, but in SimpleITK two images are considered equivalent iff their content and spatial extent are the same.\n",
    "\n",
    "The following two cells correspond to the two approaches:\n",
    "\n",
    "\n",
    "使用SimpleITK的反射可以通过以下几种方式之一完成：\n",
    "\n",
    "1. 使用仿射变换，矩阵分量设置为反射矩阵。矩阵的列对应于x，y和z轴。使用2D，3D或轴2D构建反射矩阵，我们希望通过标准基矢量ei，ej和剩余的基矢量设置为-ek来反映。\n",
    "    - 关于xy平面的反思：[e1，e2，-e3]。\n",
    "    - 关于xz平面的反思：[e1，-e2，e3]。\n",
    "    - 关于yz平面的反思：[ - e1，e2，e3]。\n",
    "2. 将图像重新采样到参考图像网格后，使用本机切片运算符（例如img [：，:: - 1，：]）或FlipImageFilter。\n",
    "我们更喜欢选项1，因为它在计算上更有效。它在重新采样之前组合所有变换，而另一种方法在参考图像网格上执行重新采样，然后进行反射操作。另一个区别是使用切片或FlipImageFilter也将修改图像原点，而重采样方法保持参考图像原点的空间位置不变。由于图像的内容相同，因此在深度学习中无关紧要，但在SimpleITK中，如果两个图像的内容和空间范围相同，则认为它们是等同的。\n",
    "\n",
    "以下两个单元格对应于两种方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n1 -r1\n",
    "# Approach 1, using an affine transformation\n",
    "\n",
    "flipped_images = []\n",
    "for index,img in enumerate(data):\n",
    "    # Compute the transformation which maps between the reference and current image (same as done above).\n",
    "    transform = sitk.AffineTransform(dimension)\n",
    "    transform.SetMatrix(img.GetDirection())\n",
    "    transform.SetTranslation(np.array(img.GetOrigin()) - reference_origin)\n",
    "    centering_transform = sitk.TranslationTransform(dimension)\n",
    "    img_center = np.array(img.TransformContinuousIndexToPhysicalPoint(np.array(img.GetSize())/2.0))\n",
    "    centering_transform.SetOffset(np.array(transform.GetInverse().TransformPoint(img_center) - reference_center))\n",
    "    centered_transform = sitk.Transform(transform)\n",
    "    centered_transform.AddTransform(centering_transform)\n",
    "    \n",
    "    flipped_transform = sitk.AffineTransform(dimension)    \n",
    "    flipped_transform.SetCenter(reference_image.TransformContinuousIndexToPhysicalPoint(np.array(reference_image.GetSize())/2.0))\n",
    "    if dimension==2: # matrices in SimpleITK specified in row major order\n",
    "        flipped_transform.SetMatrix([1,0,0,-1])\n",
    "    else:\n",
    "        flipped_transform.SetMatrix([1,0,0,0,-1,0,0,0,1])\n",
    "    centered_transform.AddTransform(flipped_transform)\n",
    "    \n",
    "    # Resample onto the reference image \n",
    "    flipped_images.append(sitk.Resample(img, reference_image, centered_transform, sitk.sitkLinear, 0.0))\n",
    "# Uncomment the following line to display the images (we don't want to time this)\n",
    "#disp_images(flipped_images, fig_size=(6,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n1 -r1\n",
    "\n",
    "# Approach 2, flipping after resampling\n",
    "\n",
    "flipped_images = []\n",
    "for index,img in enumerate(data):\n",
    "    # Compute the transformation which maps between the reference and current image (same as done above).\n",
    "    transform = sitk.AffineTransform(dimension)\n",
    "    transform.SetMatrix(img.GetDirection())\n",
    "    transform.SetTranslation(np.array(img.GetOrigin()) - reference_origin)\n",
    "    centering_transform = sitk.TranslationTransform(dimension)\n",
    "    img_center = np.array(img.TransformContinuousIndexToPhysicalPoint(np.array(img.GetSize())/2.0))\n",
    "    centering_transform.SetOffset(np.array(transform.GetInverse().TransformPoint(img_center) - reference_center))\n",
    "    centered_transform = sitk.Transform(transform)\n",
    "    centered_transform.AddTransform(centering_transform)\n",
    "    # Resample onto the reference image \n",
    "    resampled_img = sitk.Resample(img, reference_image, centered_transform, sitk.sitkLinear, 0.0)\n",
    "    # We flip on the y axis (x, z are done similarly)\n",
    "    if dimension==2:\n",
    "        flipped_images.append(resampled_img[:,::-1])\n",
    "    else:\n",
    "        flipped_images.append(resampled_img[:,::-1,:])\n",
    "# Uncomment the following line to display the images (we don't want to time this)        \n",
    "#disp_images(flipped_images, fig_size=(6,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Radial Distortion\n",
    "\n",
    "Some 2D medical imaging modalities, such as endoscopic video and X-ray images acquired with C-arms using image intensifiers, exhibit radial distortion. The common model for such distortion was described by Brown [\"Close-range camera calibration\", Photogrammetric Engineering, 37(8):855–866, 1971]:\n",
    "$$\n",
    "\\mathbf{p}_u = \\mathbf{p}_d + (\\mathbf{p}_d-\\mathbf{p}_c)(k_1r^2 + k_2r^4 + k_3r^6 + \\ldots)\n",
    "$$\n",
    "\n",
    "where:\n",
    "* $\\mathbf{p}_u$ is a point in the undistorted image\n",
    "* $\\mathbf{p}_d$ is a point in the distorted image\n",
    "* $\\mathbf{p}_c$ is the center of distortion\n",
    "* $r = \\|\\mathbf{p}_d-\\mathbf{p}_c\\|$\n",
    "* $k_i$ are coefficients of the radial distortion\n",
    "\n",
    "\n",
    "Using SimpleITK operators we represent this transformation using a deformation field as follows:\n",
    "\n",
    "一些2D医学成像模态，例如内窥镜视频和使用图像增强器的C臂获取的X射线图像，表现出径向畸变。 Brown [“近距离相机校准”，摄影测量工程，37（8）：855-866,1971]描述了这种失真的通用模型：\n",
    "PU = PD +（PD-PC）（k1r2+ k2r4+ k3r6+...）\n",
    " \n",
    "哪里：\n",
    "\n",
    "pu是未失真图像中的一个点\n",
    "pd是扭曲图像中的一个点\n",
    "pc是失真的中心\n",
    "R =‖pd-pc‖\n",
    "ki是径向畸变的系数\n",
    "使用SimpleITK运算符，我们使用变形字段表示此变换，如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def radial_distort(image, k1, k2, k3, distortion_center=None):\n",
    "    c = distortion_center\n",
    "    if not c: # The default distortion center coincides with the image center\n",
    "        c = np.array(image.TransformContinuousIndexToPhysicalPoint(np.array(image.GetSize())/2.0))\n",
    "    \n",
    "    # Compute the vector image (p_d - p_c) \n",
    "    delta_image = sitk.PhysicalPointSource( sitk.sitkVectorFloat64, image.GetSize(), image.GetOrigin(), image.GetSpacing(), image.GetDirection())\n",
    "    delta_image_list = [sitk.VectorIndexSelectionCast(delta_image,i) - c[i] for i in range(len(c))]\n",
    "    \n",
    "    # Compute the radial distortion expression\n",
    "    r2_image = sitk.NaryAdd([img**2 for img in delta_image_list])\n",
    "    r4_image = r2_image**2\n",
    "    r6_image = r2_image*r4_image\n",
    "    disp_image = k1*r2_image + k2*r4_image + k3*r6_image\n",
    "    displacement_image = sitk.Compose([disp_image*img for img in delta_image_list])\n",
    "    \n",
    "    displacement_field_transform = sitk.DisplacementFieldTransform(displacement_image)\n",
    "    return sitk.Resample(image, image, displacement_field_transform)\n",
    "\n",
    "k1 = 0.00001\n",
    "k2 = 0.0000000000001\n",
    "k3 = 0.0000000000001\n",
    "original_image = data[0]\n",
    "distorted_image = radial_distort(original_image, k1, k2, k3)\n",
    "# Use a grid image to highlight the distortion.\n",
    "grid_image = sitk.GridSource(outputPixelType=sitk.sitkUInt16, size=original_image.GetSize(), \n",
    "                             sigma=[0.1]*dimension, gridSpacing=[20.0]*dimension)\n",
    "grid_image.CopyInformation(original_image)\n",
    "distorted_grid = radial_distort(grid_image, k1, k2, k3)\n",
    "disp_images([original_image, distorted_image, distorted_grid], fig_size=(6,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transferring deformations - exercise for the interested reader\n",
    "\n",
    "Using SimpleITK we can readily transfer deformations from a spatio-temporal data set to another spatial data set to simulate temporal behavior. Case in point, using a 4D (3D+time) CT of the thorax we can estimate the respiratory motion using non-rigid registration and [Free Form Deformation](65_Registration_FFD.ipynb) or [displacement field](66_Registration_Demons.ipynb) transformations. We can then register a new spatial data set to the original spatial CT (non-rigidly) followed by application of the temporal deformations.\n",
    "\n",
    "Note that unlike the arbitrary spatial transformations we used for data-augmentation above this approach is more computationally expensive as it involves multiple non-rigid registrations. Also note that as the goal is to use the estimated transformations to create plausible deformations you may be able to relax the required registration accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentation using intensity modifications\n",
    "\n",
    "SimpleITK has many filters that are potentially relevant for data augmentation via modification of intensities. For example:\n",
    "* Image smoothing, always read the documentation carefully, similar filters use use different parametrization $\\sigma$ vs. variance ($\\sigma^2$):\n",
    "  * [Discrete Gaussian](https://itk.org/SimpleITKDoxygen/html/classitk_1_1simple_1_1DiscreteGaussianImageFilter.html)\n",
    "  * [Recursive Gaussian](https://itk.org/SimpleITKDoxygen/html/classitk_1_1simple_1_1RecursiveGaussianImageFilter.html)\n",
    "  * [Smoothing Recursive Gaussian](https://itk.org/SimpleITKDoxygen/html/classitk_1_1simple_1_1SmoothingRecursiveGaussianImageFilter.html)\n",
    "\n",
    "* Edge preserving image smoothing:\n",
    "  * [Bilateral image filtering](https://itk.org/SimpleITKDoxygen/html/classitk_1_1simple_1_1BilateralImageFilter.html), edge preserving smoothing.\n",
    "  * [Median filtering](https://itk.org/SimpleITKDoxygen/html/classitk_1_1simple_1_1MedianImageFilter.html)\n",
    "\n",
    "* Adding noise to your images:\n",
    "  * [Additive Gaussian](https://itk.org/SimpleITKDoxygen/html/classitk_1_1simple_1_1AdditiveGaussianNoiseImageFilter.html)\n",
    "  * [Salt and Pepper / Impulse](https://itk.org/SimpleITKDoxygen/html/classitk_1_1simple_1_1SaltAndPepperNoiseImageFilter.html)\n",
    "  * [Shot/Poisson](https://itk.org/SimpleITKDoxygen/html/classitk_1_1simple_1_1ShotNoiseImageFilter.html)\n",
    "  * [Speckle/multiplicative](https://itk.org/SimpleITKDoxygen/html/classitk_1_1simple_1_1SpeckleNoiseImageFilter.html)\n",
    "  \n",
    "* [Adaptive Histogram Equalization](https://itk.org/SimpleITKDoxygen/html/classitk_1_1simple_1_1AdaptiveHistogramEqualizationImageFilter.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_images_intensity(image_list, output_prefix, output_suffix):\n",
    "    '''\n",
    "    Generate intensity modified images from the originals.\n",
    "    Args:\n",
    "        image_list (iterable containing SimpleITK images): The images which we whose intensities we modify.\n",
    "        output_prefix (string): output file name prefix (file name: output_prefixi_FilterName.output_suffix).\n",
    "        output_suffix (string): output file name suffix (file name: output_prefixi_FilterName.output_suffix).\n",
    "    '''\n",
    "\n",
    "    # Create a list of intensity modifying filters, which we apply to the given images\n",
    "    filter_list = []\n",
    "    \n",
    "    # Smoothing filters\n",
    "    \n",
    "    filter_list.append(sitk.SmoothingRecursiveGaussianImageFilter())\n",
    "    filter_list[-1].SetSigma(2.0)\n",
    "    \n",
    "    filter_list.append(sitk.DiscreteGaussianImageFilter())\n",
    "    filter_list[-1].SetVariance(4.0)\n",
    "    \n",
    "    filter_list.append(sitk.BilateralImageFilter())\n",
    "    filter_list[-1].SetDomainSigma(4.0)\n",
    "    filter_list[-1].SetRangeSigma(8.0)\n",
    "    \n",
    "    filter_list.append(sitk.MedianImageFilter())\n",
    "    filter_list[-1].SetRadius(8)\n",
    "    \n",
    "    # Noise filters using default settings\n",
    "    \n",
    "    # Filter control via SetMean, SetStandardDeviation.\n",
    "    filter_list.append(sitk.AdditiveGaussianNoiseImageFilter())\n",
    "\n",
    "    # Filter control via SetProbability\n",
    "    filter_list.append(sitk.SaltAndPepperNoiseImageFilter())\n",
    "    \n",
    "    # Filter control via SetScale\n",
    "    filter_list.append(sitk.ShotNoiseImageFilter())\n",
    "    \n",
    "    # Filter control via SetStandardDeviation\n",
    "    filter_list.append(sitk.SpeckleNoiseImageFilter())\n",
    "\n",
    "    filter_list.append(sitk.AdaptiveHistogramEqualizationImageFilter())\n",
    "    filter_list[-1].SetAlpha(1.0)\n",
    "    filter_list[-1].SetBeta(0.0)\n",
    "\n",
    "    filter_list.append(sitk.AdaptiveHistogramEqualizationImageFilter())\n",
    "    filter_list[-1].SetAlpha(0.0)\n",
    "    filter_list[-1].SetBeta(1.0)\n",
    "    \n",
    "    aug_image_lists = [] # Used only for display purposes in this notebook.\n",
    "    for i,img in enumerate(image_list):\n",
    "        aug_image_lists.append([f.Execute(img) for f in filter_list])            \n",
    "        for aug_image,f in zip(aug_image_lists[-1], filter_list):\n",
    "            sitk.WriteImage(aug_image, output_prefix + str(i) + '_' +\n",
    "                            f.GetName() + '.' + output_suffix)\n",
    "    return aug_image_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the intensities of the original images using the set of SimpleITK filters described above. If we are working with 2D images the results will be displayed inline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intensity_augmented_images = augment_images_intensity(data, os.path.join(OUTPUT_DIR, 'intensity_aug'), 'mha')\n",
    "\n",
    "          # in 2D we join all of the images into a 3D volume which we use for display.\n",
    "if dimension==2:    \n",
    "    def list2_float_volume(image_list) :\n",
    "        return sitk.JoinSeries([sitk.Cast(img, sitk.sitkFloat32) for img in image_list])\n",
    "        \n",
    "    all_images = [list2_float_volume(imgs) for imgs in intensity_augmented_images]\n",
    "    \n",
    "    # Compute reasonable window-level values for display (just use the range of intensity values\n",
    "    # from the original data).\n",
    "    original_window_level = []\n",
    "    statistics_image_filter = sitk.StatisticsImageFilter()\n",
    "    for img in data:\n",
    "        statistics_image_filter.Execute(img)\n",
    "        max_intensity = statistics_image_filter.GetMaximum()\n",
    "        min_intensity = statistics_image_filter.GetMinimum()\n",
    "        original_window_level.append((max_intensity-min_intensity, (max_intensity+min_intensity)/2.0))\n",
    "    gui.MultiImageDisplay(image_list=all_images, shared_slider=True, figure_size=(6,2), window_level_list=original_window_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can easily create intensity variations that are specific to your domain, such as the spatially varying multiplicative and additive transformation shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mult_and_add_intensity_fields(original_image):\n",
    "    '''\n",
    "    Modify the intensities using multiplicative and additive Gaussian bias fields.\n",
    "    '''\n",
    "    # Gaussian image with same meta-information as original (size, spacing, direction cosine)\n",
    "    # Sigma is half the image's physical size and mean is the center of the image. \n",
    "    g_mult = sitk.GaussianSource(original_image.GetPixelIDValue(),\n",
    "                             original_image.GetSize(),\n",
    "                             [(sz-1)*spc/2.0 for sz, spc in zip(original_image.GetSize(), original_image.GetSpacing())],\n",
    "                             original_image.TransformContinuousIndexToPhysicalPoint(np.array(original_image.GetSize())/2.0),\n",
    "                             255,\n",
    "                             original_image.GetOrigin(),\n",
    "                             original_image.GetSpacing(),\n",
    "                             original_image.GetDirection())\n",
    "\n",
    "    # Gaussian image with same meta-information as original (size, spacing, direction cosine)\n",
    "    # Sigma is 1/8 the image's physical size and mean is at 1/16 of the size \n",
    "    g_add = sitk.GaussianSource(original_image.GetPixelIDValue(),\n",
    "                             original_image.GetSize(),\n",
    "               [(sz-1)*spc/8.0 for sz, spc in zip(original_image.GetSize(), original_image.GetSpacing())],\n",
    "               original_image.TransformContinuousIndexToPhysicalPoint(np.array(original_image.GetSize())/16.0),\n",
    "               255,\n",
    "               original_image.GetOrigin(),\n",
    "               original_image.GetSpacing(),\n",
    "               original_image.GetDirection())\n",
    "    \n",
    "    return g_mult*original_image+g_add\n",
    "\n",
    "disp_images([mult_and_add_intensity_fields(img) for img in data], fig_size=(6,2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
